

# ğŸ§³ Travel Concierge â€“ Multi-Agent Workflow with LangGraph

This project demonstrates a **production-style multi-agent workflow** built with [LangGraph](https://github.com/langchain-ai/langgraph).
The workflow models a **Travel Concierge** that plans a 3-day trip using live weather and places APIs, with built-in **guardrails, fallbacks, retries, and observability**.

---

## âœ¨ Features

* **Multi-Agent Collaboration**

  * **Researcher** â†’ calls weather, places, and search APIs.
  * **Planner** â†’ parses the query into structured trip plan.
  * **Executor** â†’ generates final itinerary using LLM (Groq).
  * **Reviewer** â†’ validates schema & repairs invalid outputs.
* **Guardrails**

  * Prompt hardening in agent prompts (forbids secret exfiltration, misuse, jailbreaks).
  * Schema validation with **Pydantic** (`ItineraryOutput`).
  * Toxicity / policy checks (lightweight moderation step).
* **Resilience**

  * Tool retries with exponential backoff.
  * Per-node fallbacks (executor has minimal itinerary fallback).
  * Circuit breaker pattern if repeated failures.
* **Observability**

  * Integrated with **LangSmith** for tracing, tokens, and latency tracking.
  * Example traces exported to `artifacts/sample_trace.json`.
* **(Optional)** MCP Tool integration (filesystem/OpenAPI stubs).

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ graph.py              # LangGraph orchestration
â”‚   â”œâ”€â”€ state.py              # Shared state definition
â”‚   â”œâ”€â”€ agents
â”‚   â”‚   â”œâ”€â”€ researcher.py     # Weather + places lookup
â”‚   â”‚   â”œâ”€â”€ planner.py        # Parse query â†’ structured plan
â”‚   â”‚   â”œâ”€â”€ executor.py       # Generate itinerary JSON
â”‚   â”‚   â””â”€â”€ reviewer.py       # Schema validation & repair
â”‚   â”œâ”€â”€ tools
â”‚   â”‚   â”œâ”€â”€ search.py         # Serper API
â”‚   â”‚   â”œâ”€â”€ weather.py        # OpenWeather API (RapidAPI)
â”‚   â”‚   â””â”€â”€ places.py         # Maps Places API (RapidAPI)
â”‚   â”œâ”€â”€ guardrails
â”‚   â”‚   â”œâ”€â”€ schemas.py        # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ pii.py            # (stub) PII redaction
â”‚   â”‚   â””â”€â”€ moderation.py     # (stub) toxicity checks
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â””â”€â”€ retry.py          # Retry & fallback logic
â”‚   â”œâ”€â”€ fallbacks.py          # Node & circuit breaker fallbacks
â”‚   â””â”€â”€ observability.py      # LangSmith tracing setup
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ demo.ipynb            # Interactive demo
â”œâ”€â”€ artifacts
â”‚   â””â”€â”€ sample_trace.json     # Example run trace
â”œâ”€â”€ README.md                 # Project docs
â””â”€â”€ .env.example              # Example environment variables
```

---

## ğŸš€ Setup

1. **Clone Repo**

   ```bash
   git clone https://github.com/<your-org>/travel-concierge-langgraph.git
   cd travel-concierge-langgraph
   ```

2. **Install Dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Configure Environment**
   Copy `.env.example` â†’ `.env` and fill in:

   ```
   GROQ_API_KEY=your_groq_key
   SERPER_API_KEY=your_serper_key
   RAPIDAPI_KEY=your_rapidapi_key
   LANGCHAIN_API_KEY=your_langsmith_key
   LANGCHAIN_PROJECT=travel-concierge
   ```

4. **Run the Graph**

   ```bash
   python -m src.graph
   ```

---

## ğŸ§ª Example Run

Input:

```python
inputs = {"query": "plan a 3-day trip to New York"}
result = graph.invoke(inputs)
print(json.dumps(result["outputs"], indent=2))
```

Output (fallback example):

```json
{
  "city": "New York",
  "days": [
    {
      "day": 1,
      "activities": [
        {"time": "morning", "place": "Central Park", "type": "sightseeing"},
        {"time": "evening", "place": "Local Restaurant", "type": "restaurant"}
      ]
    }
  ],
  "weather_forecast": "Data unavailable, please check manually."
}
```

---

## ğŸ”’ Guardrails

1. **Prompt Hardening**

   * All system prompts forbid jailbreaks, secret exfiltration, or unsafe tool use.
   * High-risk patterns (e.g. "ignore instructions") are explicitly blocked.

2. **Schema Validation**

   * Every cross-node output must match `ItineraryOutput`.
   * If invalid â†’ violation is logged and fallback triggered.

3. **Moderation Check**

   * Outputs are scanned for toxic / unsafe text.
   * If triggered â†’ routed to Reviewer or human-in-the-loop.

---

## ğŸ” Resilience

* **Per-tool retries** with exponential backoff (max 2).
* **Per-node fallback** (Executor falls back to minimal itinerary).
* **Circuit breaker** if too many global failures â†’ graceful summary with apology.

---

## ğŸ“Š Observability

* **LangSmith integration**: traces, metrics, and run artifacts.
* Metrics summary (example run):

  * Token usage: \~1.2k
  * Avg tool latency: 850ms
  * Failure count: 1
  * Fallback rate: 33%

See: `artifacts/sample_trace.json`

---

## ğŸ¥ Demo Video

* **Happy path** â†’ normal API calls + structured itinerary.
* **Failure path** â†’ API error triggers fallback.
  (Video placeholder here â€“ add your 3-min demo recording.)

---



---



-

Would you like me to **include explicit example system prompts** (the â€œprompt hardeningâ€ ones you need for the report), or keep README high-level and leave them in `src/agents/` docstrings?
